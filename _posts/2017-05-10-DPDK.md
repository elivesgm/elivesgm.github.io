---
layout: post
title: "DPDK"
date: 2017-05-10 18:15:06 
description: "Open source project"
tag: Open Source
---

### 1 Motivation

主要参考文章：http://www.jianshu.com/p/0ff8cb4deaef

Linux内核协议栈：

[1] http://www.jianshu.com/p/0ff8cb4deaef

[2] http://blog.csdn.net/column/details/network-kernel-yoyo.html

[3] http://blog.csdn.net/xaiojiang/article/details/51584533

Way Out:

- 控制层留给Linux做，其它数据层全部由应用程序来处理。
- 减少系统调度、系统调用、系统中断，上下文切换等
- 摒弃Linux内核协议栈，将数据包传输到用户空间定制协议栈
- 使用多核编程技术替代多线程，将OS绑在指定核上运行
- 针对SMP系统，使CPU尽量使用所在NUMA系统节点的内存，减少内存刷写
- 使用大页面，减少访问
- 采用无锁技术解竞争

### 2 简介

DPDK是一个快速包处理的lib库和驱动的集合。适用于任何处理器。

Main libraries

- multicore framework
- huge page memory
- ring buffers
- poll-mode drivers for networking, crypto and eventdev

These libraries can be used to:

- receive and send packets within the minimum number of CPU cycles (usually less than 80 cycles)
- develop fast packet capture algorithm
- run third-party fast path stacks

DPDK应用程序是运行在用户空间上利用自身提供的数据平面库来收发数据包，绕过了Linux内核协议栈对数据包处理过程。Linux内核将DPDK应用程序看作是一个普通的用户态进程，包括它的编译、连接和加载方式和普通程序没有什么两样。

总体结构：

- 网络层模块
- 内存管理模块
- 内核管理模块

### 3. 网络模块

####3.1 传统Linux网络流程 v.s. DPDK

传统linux网络层:

    硬件中断--->取包分发至内核线程--->软件中断--->内核线程在协议栈中处理包--->处理完毕通知用户层
    用户层收包-->网络层--->逻辑层--->业务层

dpdk网络层:

    硬件中断--->放弃中断流程
    用户层通过设备映射取包--->进入用户层协议栈--->逻辑层--->业务层

DPDK针对Intel网卡实现了基于轮询方式的PMD（Poll Mode Drivers）驱动，该驱动由API、用户空间运行的驱动程序构成，该驱动使用无中断方式直接操作网卡的接收和发送队列（除了链路状态通知仍必须采用中断方式以外）。目前PMD驱动支持Intel的大部分1G、10G和40G的网卡。PMD驱动从网卡上接收到数据包后，会直接通过DMA方式传输到预分配的内存中，同时更新无锁环形队列中的数据包指针，不断轮询的应用程序很快就能感知收到数据包，并在预分配的内存地址上直接处理数据包，这个过程非常简洁。如果要是让Linux来处理收包过程，首先网卡通过中断方式通知协议栈对数据包进行处理，协议栈先会对数据包进行合法性进行必要的校验，然后判断数据包目标是否本机的socket，满足条件则会将数据包拷贝一份向上递交给用户socket来处理，不仅处理路径冗长，还需要从内核到应用层的一次拷贝过程。

DPDK通过在线程中使用epoll模型，监听UIO设备的事件，来模拟操作系统的中断处理。

#### 3.2 UIO, Userspace I/O

将驱动的很少一部分运行在内核空间，而在用户空间实现驱动的绝大多数功能。UIO的内核部分和用户空间的工作：
a. 内核空间:

- 分配和记录设备需要的资源和注册uio设备
- 在内核空间实现的小部分中断应答函数

b. 用户空间:


### 4 内存管理

#### 4.1 hugepage技术
DPDK目前支持了2M和1G两种方式的hugepage。

- 不容易缺页,减少CPU TLB表的Miss次数
- 减少页表项，缺页时可以快速找到

#### 4.2 对齐

- 内存对齐
- cache对齐，Cache line是CPU从内存加载数据的最小单位，一般L1 cache的cache line大小为64字节。如果CPU访问的变量不在cache中，就需要先从内存调入到cache，调度的最小单位就是cache line。因此，内存访问如果没有按照cache line边界对齐，就会多读写一次内存和 cache了。

DPDK的线程分为控制线程和数据线程，控制线程一般绑定到MASTER核上，主要是接受用户配置，并传递配置参数给数据线程等；数据线程主要是处理数据包。

#### 4.3 无锁环形缓冲区

### 5 内核管理
基于NUMA的绑核。

#### 5.1 NUMA, Non-Uniform Memory Access
从系统架构来看，目前的商用服务器大体可以分为三类，即对称多处理器结构 (SMP, Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA, Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP, Massive Parallel Processing) 。

a. SMP

所有的CPU共享全部资源，如总线，内存和I/O系统等。CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。

b. NUMA

NUMA服务器的基本特征是具有多个 CPU 模块，每个 CPU 模块由多个 CPU( 如 4 个 ) 组成，并且具有独立的本地内存、 I/O 槽口等。由于其节点之间可以通过互联模块 ( 如称为 Crossbar Switch) 进行连接和信息交互，因此每个 CPU 可以访问整个系统的内存 ( 这是 NUMA 系统与 MPP 系统的重要差别 ) 。显然，访问本地内存的速度将远远高于访问远地内存 ( 系统内其它节点的内存 ) 的速度，这也是非一致存储访问 NUMA 的由来。由于这个特点，为了更好地发挥系统性能，开发应用程序时需要尽量减少不同 CPU 模块之间的信息交互。

c. MPP

和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构


#### 5.2 CPU亲和性

CPU的亲和性也就是cpu affinity机制，指的是进程要在指定的 CPU 上尽量长时间地运行而不被迁移到其他处理器, 通过处理器关联可以将虚拟处理器映射到一个或多个物理处理器上，也就是说把一个程序绑定到一个物理CPU上。

在越来越多核心的cpu机器上，如何提高外设以及程序工作效率的最直观想法就是让各个cpu核心各自干专门的事情。而且在多核运行的机器上，每个CPU本身自己会有缓存，缓存着进程使用的信息，而进程可能会被OS调度到其他CPU上，如此，CPU cache命中率就低了，当绑定CPU后，程序就会一直在指定的cpu跑，不会由操作系统调度到其他CPU上，性能有一定的提高。

另外一种使用绑核考虑就是将重要的业务进程隔离开，对于部分实时进程调度优先级高，可以将其绑定到一个指定核上，既可以保证实时进程的调度，也可以避免其他CPU上进程被该实时进程干扰。

DPDK利用cpu affinity主要是将控制面线程以及各个数据面线程绑定到不同的cpu，省却了来回反复调度的性能消耗，线程之间互不干扰的完成工作。




### Reference

[1] [DPDK](http://feisky.xyz/sdn/dpdk/),feisky.xyz.

[2] [DPDK分析](http://www.jianshu.com/p/0ff8cb4deaef%20%20),jianshu.com.

[3] [DPDK](http://www.cnblogs.com/MerlinJ/tag/DPDK/),cnblogs.com.

[4] [DPDK简介](http://blog.csdn.net/divlee130/article/category/5634039),csdn.net.

[5] [SMP、NUMA、MPP体系结构介绍](http://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html),cnblogs.com.

[6] [DPDK中的无锁环形缓冲区](https://titenwang.github.io/2016/11/20/dpdk-ring-buffer/),github.io.